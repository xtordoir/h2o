{
  "metadata" : {
    "name" : "PopulationStratification",
    "user_save_timestamp" : "1970-01-01T01:00:00.000Z",
    "auto_save_timestamp" : "1970-01-01T01:00:00.000Z",
    "language_info" : {
      "name" : "scala",
      "file_extension" : "scala",
      "codemirror_mode" : "text/x-scala"
    },
    "trusted" : true,
    "customLocalRepo" : "/root/.m2/repository",
    "customRepos" : null,
    "customDeps" : [ "ai.h2o % sparkling-water-core_2.10 % 1.3.7", "ai.h2o % h2o-algos % 3.0.0.26", "org.bdgenomics.adam % adam-core % 0.16.0", "guru.datafellas % variant_2.10 % 0.1.0", "org.scala-lang % jline % 2.10.4", "- log4j % log4j % 1.2.15", "- org.apache.hadoop % hadoop-client %   _", "- org.apache.spark  % spark-core_2.10    %   _", "- org.apache.spark % spark-mllib_2.10 % _", "- org.apache.spark % spark-repl_2.10 % _", "- org.scala-lang    %     _         %   _", "- org.scoverage     %     _         %   _" ],
    "customImports" : null,
    "customArgs" : null,
    "customSparkConf" : {
      "spark.executor.memory" : "13g"
    }
  },
  "cells" : [ {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "## Import bunch of dependencies"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "import hex.FrameSplitter\nimport hex.deeplearning.DeepLearning\nimport hex.deeplearning.DeepLearningParameters\nimport hex.deeplearning.DeepLearningParameters.Activation\nimport org.apache.spark.{SparkConf, SparkContext}\nimport org.apache.spark.SparkContext.rddToPairRDDFunctions\nimport org.apache.spark.h2o.H2OContext\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.types._\nimport org.bdgenomics.adam.rdd.ADAMContext._\nimport org.bdgenomics.formats.avro.{Genotype, GenotypeAllele}\nimport water.Key\n\nimport scala.collection.JavaConverters._\nimport scala.collection.immutable.Range.inclusive\nimport scala.io.Source\n\nimport guru.datafellas._\n",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "import sys.process._\nval master = (\"ec2-metadata --public-hostname\"!!).drop(\"public-hostname: \".size).mkString.trim\ndef hu(s:String) = s\"hdfs://$master:9010/temp/$s\"",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true
    },
    "cell_type" : "code",
    "source" : "val fs_s3_awsAccessKeyId      = sys.env.get(\"AWS_ACCESS_KEY_ID\").getOrElse(\"<hard-code-one>\")\nval fs_s3_awsSecretAccessKey  = sys.env.get(\"AWS_SECRET_ACCESS_KEY\").getOrElse(\"<hard-code-one>\")\n()",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "reset(lastChanges = _.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n                     .set(\"spark.kryo.registrator\", \"guru.datafellas.VariantKryoRegistrator\")\n                     .set(\"spark.kryoserializer.buffer.mb\", \"4\")\n                     .set(\"spark.kryo.referenceTracking\", \"true\")\n                     .setMaster(s\"spark://$master:7077\")\n                     .setAppName(\"H2O-ADAM\")\n                     .set(\"spark.executor.memory\", \"13g\")\n                     .set(\"fs.s3.awsAccessKeyId\", fs_s3_awsAccessKeyId)\n                     .set(\"fs.s3.awsSecretAccessKey\", fs_s3_awsSecretAccessKey)\n)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val genotypeFile = \"s3n://med-at-scale/1000genomes/ALL.chr22.integrated_phase1_v3.20101123.snps_indels_svs.genotypes.vcf.adam\"\n//val genotypeFile = \"/Users/xavier/data/chr22-16m-18m.adam\"",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val allGenotypes: RDD[Genotype] = sc.loadGenotypes(genotypeFile)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val start = 16000000\nval end   = 26000000",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val snpgts = allGenotypes.filter(g => (g.getVariant.getStart >= start && g.getVariant.getEnd <= end) )\n                         .filter{g => \n               val bases = Set(\"A\",\"T\",\"G\",\"C\")\n               bases.contains(g.getVariant.getReferenceAllele) && \n                                      bases.contains(g.getVariant.getAlternateAllele)\n          }",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val pops = Set(\"GBR\", \"ASW\", \"CHB\")",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "### Get the populations"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "import sys.process._",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val panelFile = \"/vol0/ALL.panel\"",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "s\"wget ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/integrated_call_samples_v3.20130502.ALL.panel -O ${panelFile}\"!!",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "import scala.io.Source\ndef extract(filter: (String, String) => Boolean= (s, t) => true) = Source.fromFile(panelFile).getLines().map( line => {\n  val toks = line.split(\"\\t\").toList\n  toks(0) -> toks(1)\n}).toMap.filter( tup => filter(tup._1, tup._2) )\n  \n// panel extract from file, filtering by the 2 populations\nval panel: Map[String,String] = \n  extract((sampleID: String, pop: String) => pops.contains(pop)) \n  \n// broadcast the panel \nval bPanel = sparkContext.broadcast(panel)\n()",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val finalGts = snpgts.filter(g =>  bPanel.value.contains(g.getSampleId))",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "//case class SampleVariant(sampleId: String, variantId: Int, alternateCount: Int) extends java.io.Serializable",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "//case class SampleVariant(sampleId: String, variantId: Int, alternateCount: Int) \n\n\ndef variantId(genotype: Genotype): String = {\n      val name = genotype.getVariant.getContig.getContigName\n      val start = genotype.getVariant.getStart\n      val end = genotype.getVariant.getEnd\n      s\"$name:$start:$end\"\n    }\ndef alternateCount(genotype: Genotype): Int = {\n  genotype.getAlleles.asScala.count(_ != GenotypeAllele.Ref)\n}\n    def toVariant(genotype: Genotype): SampleVariant = {\n      // Intern sample IDs as they will be repeated a lot\n      new SampleVariant(genotype.getSampleId.intern(), variantId(genotype).hashCode(), alternateCount(genotype))\n    }\n",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val variantsRDD: RDD[SampleVariant] = finalGts.map(toVariant)\nvariantsRDD.cache()",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val variantsBySampleId: RDD[(String, Iterable[SampleVariant])] = variantsRDD.groupBy{v: SampleVariant => v.sampleId}\nval sampleCount: Long = variantsBySampleId.count()\nprintln(\"Found \" + sampleCount + \" samples\")\nval variantsByVariantId: RDD[(Int, Iterable[SampleVariant])] = variantsRDD.groupBy(_.variantId).filter {\n  case (_, sampleVariants) => sampleVariants.size == sampleCount\n}",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "    val variantFrequencies: collection.Map[Int, Int] = variantsByVariantId.map {\n      case (variantId, sampleVariants) => (variantId, sampleVariants.count(_.alternateCount > 0))\n    }.collectAsMap()\n    val permittedRange = inclusive(10, 20)\n    val filteredVariantsBySampleId: RDD[(String, Iterable[SampleVariant])] = variantsBySampleId.map {\n      case (sampleId, sampleVariants) =>\n        val filteredSampleVariants = sampleVariants.filter(variant => permittedRange.contains(\n          variantFrequencies.getOrElse(variant.variantId, -1)))\n        (sampleId, filteredSampleVariants)\n    }",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "    val sortedVariantsBySampleId: RDD[(String, Array[SampleVariant])] = filteredVariantsBySampleId.map {\n      case (sampleId, variants) =>\n        (sampleId, variants.toArray.sortBy(_.variantId))\n    }",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "import org.apache.spark.sql.types._\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.Row\n",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val sqlContext = new org.apache.spark.sql.SQLContext(sc)\nimport sqlContext.implicits._",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val popToInt = Map(\"GBR\" -> 0, \"CHB\" -> 1, \"ASW\" -> 2, \"Unknown\" -> -1)\n\nval header = StructType(Array(StructField(\"Region\", StringType)) ++\n      sortedVariantsBySampleId.first()._2.map(variant => {StructField(variant.variantId.toString, IntegerType)}))\nval rowRDD: RDD[Row] = sortedVariantsBySampleId.map {\n    case (sampleId, sortedVariants) =>\n      val region: Array[String] = Array(panel.getOrElse(sampleId, \"Unknown\"))\n      val alternateCounts: Array[Int] = sortedVariants.map(_.alternateCount)\n      Row.fromSeq(region ++ alternateCounts)\n}\n",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val df = sqlContext.createDataFrame(rowRDD, header)\n()",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true
    },
    "cell_type" : "code",
    "source" : "df.write.save(\"/Users/xavier/data/df.parquet\")",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true
    },
    "cell_type" : "code",
    "source" : "df.show",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val h2oContext = new H2OContext(sc).start()\n",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "<iframe src=\"http://ec2-54-170-224-5.eu-west-1.compute.amazonaws.com:54321/perfbar.html\"></iframe>"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "df.count",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val hf = h2oContext.asH2OFrame(df)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "import scala.collection.JavaConverters._\nval keyarr= Array(\"training\", \"test\", \"validation\").map(s => Key.make(s))\nval frameSplitter = new FrameSplitter(hf, Array(.5, .3), keyarr, null)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true
    },
    "cell_type" : "code",
    "source" : "/* for h2o 3.2 ? */\n//import scala.collection.JavaConverters._\n//val arr = \n//         Array(\"training\", \"test\", \"validation\").map(s => Key.make[water.fvec.Frame](s))\n//val frameSplitter = new FrameSplitter(hf, Array(.5, .3), arr, null)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "water.H2O.submitTask(frameSplitter)\nval splits = frameSplitter.getResult\nval training = splits(0)\nval validation = splits(2)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val deepLearningParameters = new DeepLearningParameters()",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "deepLearningParameters._train = training._key",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "deepLearningParameters._valid = validation._key\ndeepLearningParameters._response_column = \"Region\"\ndeepLearningParameters._epochs = 10\ndeepLearningParameters._activation = Activation.RectifierWithDropout\ndeepLearningParameters._hidden = Array[Int](100,100)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val deepLearning = new DeepLearning(deepLearningParameters)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val deepLearningModel = deepLearning.trainModel.get",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "def confusion(data: water.fvec.Frame, model: hex.deeplearning.DeepLearningModel) = {\n  val scores = model.score(data)\n  val predf = h2oContext.asDataFrame(new water.fvec.H2OFrame(scores))(sqlContext)\n  val valRDD = h2oContext.asDataFrame(new water.fvec.H2OFrame(data))(sqlContext)\n          .map(r => r.getAs[String](0))\n  predf.rdd.map{row => \n              val cl = (1 to 3).map(row.get(_).asInstanceOf[Double]).zipWithIndex.maxBy(_._1)._2\n              (row.get(0).asInstanceOf[String], cl) \n             }\n      .map(_._2)\n      .zip(valRDD)\n      .collect.toList\n      .groupBy(_._2).mapValues(_.map(_._1))\n      .mapValues(xs => (1 to 3).map( i => xs.count(_ == i-1)).toList)\n}",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val scores = deepLearningModel.score(validation)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val predf = h2oContext.asDataFrame(new water.fvec.H2OFrame(scores))(sqlContext)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "predf.rdd.map{row => \n              val cl = (1 to 3).map(row.get(_).asInstanceOf[Double]).zipWithIndex.maxBy(_._1)._2\n              (row.get(0).asInstanceOf[String], cl) \n             }\n      .map(_._2)\n      .zip(valRDD)\n      .collect.toList\n      .groupBy(_._2).mapValues(_.map(_._1))\n      .mapValues(xs => (1 to 3).map( i => xs.count(_ == i-1)).toList)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val valRDD = h2oContext.asDataFrame(new water.fvec.H2OFrame(validation))(sqlContext)\n          .map(r => r.getAs[String](0))",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val confMat = confusion(validation, deepLearningModel)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "<table>\n<tr><td></td><td>#0</td><td>#1</td><td>#2</td></tr>\n{ for (popu <- confMat) yield\n  <tr><td>{popu._1}</td> { for (cnt <- popu._2) yield \n    <td>{cnt}</td>\n  }\n  </tr>\n}\n</table>",
    "outputs" : [ ]
  } ],
  "nbformat" : 4
}